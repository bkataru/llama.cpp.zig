const std = @import("std");

/// Formats a multiline prompt template with a varying number of dynamic string arguments as substitutions
///
/// The template is expected to contain "{s}" placeholders where the dynamic arguments
/// should be inserted. Each line of the template is treated as a potential insertion point.
///
/// Returns an allocated string containing the formatted prompt.
/// Caller owns the returned memory.
pub fn formatPromptTemplate(allocator: std.mem.Allocator, template: []const u8, substitutions: []const []const u8) ![]u8 {
    var list = std.ArrayList(u8).init(allocator);
    errdefer list.deinit();

    var arg_index: usize = 0;
    var it = std.mem.splitScalar(u8, template, '\n'); // Split the template by newline characters

    while (it.next()) |line| {
        // Iterate through each line of the template
        var line_it = std.mem.splitSequence(u8, line, "{s}"); // Split each line by the "{s}" placeholder
        try list.writer().print("{s}", .{line_it.next().?}); // Print the first part of the line

        while (line_it.next()) |part| {
            // If there's a dynamic argument available, print it
            if (arg_index < substitutions.len) {
                try list.writer().print("{s}", .{substitutions[arg_index]});
                arg_index += 1;
            }
            try list.writer().print("{s}", .{part}); // Print the next part of the line
        }
        try list.writer().writeByte('\n'); // Add a newline after each line is processed
    }
    _ = list.pop(); // Remove the last (unnecessary) newline added by the loop

    return list.toOwnedSlice();
}

pub fn main() !void {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    var debug_allocator = gpa.allocator();

    defer {
        if (gpa.deinit() == .leak) {
            std.debug.print("Memory leak detected\n", .{});
            std.process.exit(1);
        }
    }

    const template =
        \\foo is apparently {s}
        \\and bar is {s} lol
    ;

    const result = try formatPromptTemplate(debug_allocator, template, &[_][]const u8{ "hello", "world" });
    defer debug_allocator.free(result);
    std.debug.print("{s}\n", .{result});

    // Create the client
    var client = std.http.Client{ .allocator = debug_allocator };
    defer client.deinit();

    // Initialize an array list that we will use for storage of the response body
    var body = std.ArrayList(u8).init(debug_allocator);
    defer body.deinit();

    // Parse a URI for the POST endpoint
    const uri = try std.Uri.parse("http://127.0.0.1:1337/v1/chat/completions");

    // Prepare request payload
    const payload =
        \\{
        \\  "model": "Qwen_Qwen3-4B-Instruct-2507-IQ4_XS",
        \\  "messages": [
        \\    { "role": "system", "content": "You are a helpful assistant." },
        \\    { "role": "user", "content": "Hello!" }
        \\  ]
        \\}
    ;

    // Make the POST request
    const response = try client.fetch(.{
        .method = .POST,
        .location = .{ .uri = uri },
        .response_storage = .{ .dynamic = &body },
        .payload = payload,
        .headers = .{ .content_type = .{ .override = "application/json" }, .accept_encoding = .{ .override = "application/json" }, .authorization = .{ .override = "Bearer nostylist" } },
    });

    // Do whatever you need to in case of HTTP error.
    if (response.status != .ok) {
        std.debug.print("HTTP Error: {}\n", .{response.status});
        std.debug.print("Response body: {s}\n", .{body.items});
        std.debug.panic("Error connecting to llama-server: {s}", .{body.items});
    }

    // DTO for deserialization
    const LLMResponse = struct {
        id: []const u8, // Unique identifier for the response
        object: []const u8, // Type of object returned
        created: u32, // Unix timestamp of when the response was generated
        model: []const u8, // Name of the model used to generate the response
        usage: ?struct { // Usage statistics for the response, optional
            prompt_tokens: u32, // Number of tokens in the prompt
            completion_tokens: u32, // Number of tokens in the completion
            total_tokens: u32, // Total number of tokens used
        } = null,
        timings: ?struct {
            prompt_n: u32,
            prompt_ms: f64,
            prompt_per_token_ms: f64,
            prompt_per_second: f64,
            predicted_n: u32,
            predicted_ms: f64,
            predicted_per_token_ms: f64,
            predicted_per_second: f64,
        } = null,
        choices: []struct { // Array of choices generated by the model
            message: struct { // Message generated by the model
                role: []const u8,
                content: []const u8,
            },
            logprobs: ?struct { // Log probabilities of the tokens generated, optional
                content: []struct { // Array of token logprob objects
                    token: []const u8, // Token ID or string representation of the token
                    logprob: f64, // Using f64 for double precision log probabilities
                    bytes: []const u8, // Raw bytes of the token
                    // top_logprobs is an array of objects, each containing a token and its logprob
                    // This is present only if top_logprobs was requested in the API call
                    top_logprobs: ?[]struct {
                        token: []const u8,
                        logprob: f64,
                    },
                },
            } = null,
            finish_reason: []const u8, // Reason for finishing the response
            index: u32, // Index of the choice in the array
        },
        system_fingerprint: []const u8,
    };

    std.debug.print("Raw JSON response:\n{s}\n", .{body.items});
    std.debug.print("==================\n", .{});

    // Deserialize JSON response into a struct
    const parsed = try std.json.parseFromSlice(LLMResponse, debug_allocator, body.items, .{
        .allocate = .alloc_always,
        .parse_numbers = true,
        .ignore_unknown_fields = true,
        .duplicate_field_behavior = .use_last,
    });
    defer parsed.deinit();

    // Print the output
    const content = parsed.value.choices[0].message.content;
    try std.io.getStdOut().writer().print("Assistant: {s}\n", .{content});
}
